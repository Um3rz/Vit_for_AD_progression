{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11247816,"sourceType":"datasetVersion","datasetId":7024985},{"sourceId":11588136,"sourceType":"datasetVersion","datasetId":7266109}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Alzheimer's Detection Model with Swin\n\n## Overview\nThis TensorFlow/Keras model detects Alzheimer’s disease using MRI images. It uses transfer learning with a pre-trained MobileNetV2 as the base network and augments it with a custom Vision Transformer (ViT) head. Training is performed in two phases: initial training with a frozen base model and a fine-tuning phase where certain layers are unfrozen.\n\n## Data Pipeline\n- **Dataset Download:**  \n  The dataset is downloaded using `kagglehub` and is expected to contain PNG images sorted into folders by diagnosis.\n- **Label Mapping:**  \n  Two mappings are defined:\n  - Numeric-to-diagnosis (e.g., 0: 'CN', 4: 'AD').\n  - Folder name to numeric label.\n- **Image Processing:**  \n  Images are read from disk, decoded, resized to 160×160, normalized, and their labels are one-hot encoded.\n- **Data Pipeline:**  \n  Constructed using the `tf.data` API, with operations including shuffling, mapping, caching, batching, and prefetching for efficient training.\n\n## Model Architecture\n- **Base Network (MobileNetV2):**  \n  - Pre-trained on ImageNet (without the top classifier).\n  - Input: 160×160×3 images.\n  - **Layer Freezing:** Approximately 90% of the layers are frozen during the initial phase.\n  \n- **Vision Transformer (ViT) Head:**  \n  - **Projection Layer:** Projects patch features into a defined embedding space.\n  - **Class Token:** A learnable token is prepended to the patch sequence.\n  - **Positional Embeddings:** Learnable positional information is added.\n  - **Transformer Blocks:** Multiple blocks consisting of layer normalization, multi-head self-attention, MLP layers (with gelu activation and dropout), and residual connections.\n  - **Output:** The final class token is extracted as the representative feature.\n\n- **Classification Head:**\n  - A Dropout layer (50%) is applied.\n  - A Dense layer with softmax activation outputs class probabilities (for 5 classes).\n\n## Training Strategy\n- **Phase 1: Training with Frozen Base**\n  - **Objective:** Train only the top layers (ViT head and classifier) while keeping most of the MobileNetV2 weights fixed.\n  - **Optimizer:** Adam with a learning rate of 1e-4.\n  - **Loss:** Categorical crossentropy.\n  - **Epochs:** 5 (initial phase; adjustable).\n  \n- **Phase 2: Fine-Tuning**\n  - **Objective:** Unfreeze the last 10% of the MobileNetV2 layers to fine-tune the model.\n  - **Optimizer:** Adam with a reduced learning rate of 1e-5.\n  - **Epochs:** Additional 5 epochs (adjustable).\n\n## Evaluation and Visualization\n- **Callbacks:**  \n  EarlyStopping and ModelCheckpoint are used to prevent overfitting and store the best model based on validation loss.\n- **Training Visualization:**  \n  A plotting function displays the accuracy and loss curves for both phases.\n- **Final Evaluation:**  \n  The model is evaluated on the validation set, and the final validation accuracy is reported.\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks\nfrom tensorflow.keras.applications import MobileNetV2,ResNet50\nfrom tensorflow.keras.mixed_precision import set_global_policy\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport kagglehub\n\nimport os\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\nfrom tfswin import SwinTransformerTiny224","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:34:05.821546Z","iopub.execute_input":"2025-04-30T18:34:05.821877Z","iopub.status.idle":"2025-04-30T18:34:05.826877Z","shell.execute_reply.started":"2025-04-30T18:34:05.821856Z","shell.execute_reply":"2025-04-30T18:34:05.826122Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# !pip install tfswin\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nset_global_policy('mixed_float16')\ndata_dir = \"/kaggle/input/adni-for-ad-progression\"\nprint(f\"Dataset downloaded to: {data_dir}\")\n\ndiagnosis_mapping = {0: 'CN', 1: 'MCI', 2: 'EMCI', 3: 'LMCI', 4: 'AD'}\ndir_to_code       = {v: k for k, v in diagnosis_mapping.items()}\n\ndef get_image_paths_and_labels():\n    image_paths, labels = [], []\n    images_dir = os.path.join(data_dir, 'ADNI_IMAGES', 'png_images')\n    for cls, code in dir_to_code.items():\n        folder = os.path.join(images_dir, cls)\n        if os.path.exists(folder):\n            for f in os.listdir(folder):\n                if f.lower().endswith('.png'):\n                    image_paths.append(os.path.join(folder, f))\n                    labels.append(code)\n    return np.array(image_paths), np.array(labels)\n\n\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    image_paths, labels,\n    test_size=0.2,\n    random_state=42,\n    stratify=labels\n)\nprint(f\"\\nTraining:   {len(train_paths)} images\")\nprint(f\"Validation: {len(val_paths)} images\")\nIMAGE_SIZE = (160, 160)\nBATCH_SIZE  = 32\nNUM_CLASSES = len(diagnosis_mapping)\n\ndef decode_image(path, label):\n    img =tf.io.read_file(path)\n    img =tf.image.decode_png(img, channels=3)\n    img =tf.image.resize(img, IMAGE_SIZE)\n    img =tf.cast(img, tf.float32) / 255.0\n    lbl =tf.one_hot(label, NUM_CLASSES)\n    return img, lbl\n\ntrain_ds = (\n    tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n      .shuffle(len(train_paths))\n      .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n      .cache()\n      .batch(BATCH_SIZE)\n      .prefetch(tf.data.AUTOTUNE)\n)\nval_ds = (\n    tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n      .map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n      .cache()\n      .batch(BATCH_SIZE)\n      .prefetch(tf.data.AUTOTUNE)\n)\n#using swin as backbone now different approach from before this would be another model in our ensemble models\nswin_backbone = SwinTransformerTiny224(\n    include_top=False,       \n    weights='imagenet',    \n    input_shape=(*IMAGE_SIZE, 3),\n)\n\ninputs = layers.Input(shape=(*IMAGE_SIZE, 3))\nx =swin_backbone(inputs)#4d map\nx =layers.GlobalAveragePooling2D()(x)\nx =layers.Dropout(0.5)(x)\noutputs = layers.Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n\nmodel= models.Model(inputs, outputs)\nmodel.compile(\n    optimizer=optimizers.Adam(1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n    jit_compile=True,\n)\nearly_stop = callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nckpt1= callbacks.ModelCheckpoint('alz_swin_phase1.keras', monitor='val_loss', save_best_only=True)\n\nprint(\"\\nPhase 1: Training with Swin frozen\")\nhistory1 = model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[early_stop, ckpt1])\n\nfor layer in swin_backbone.layers:\n    layer.trainable = True\n\nmodel.compile(\n    optimizer=optimizers.Adam(1e-5),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n    jit_compile=True,\n)\nckpt2 = callbacks.ModelCheckpoint('alz_swin_phase2.keras', monitor='val_loss', save_best_only=True)\n\nprint(\"\\nPhase 2: Fine-tuning full Swin\")\nhistory2 = model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[early_stop, ckpt2])\n\nmodel.save('final_alzheimer_swin.keras')\n\ndef plot_history(h1, h2):\n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1,2,1)\n    plt.plot(h1.history['accuracy'], label='Phase1 Train')\n    plt.plot(h1.history['val_accuracy'], label='Phase1 Val')\n    off = len(h1.history['accuracy'])\n    x2  = [off + i for i in range(len(h2.history['accuracy']))]\n    plt.plot(x2, h2.history['accuracy'], label='Phase2 Train')\n    plt.plot(x2, h2.history['val_accuracy'], label='Phase2 Val')\n    plt.title('Accuracy'); plt.xlabel('Epoch'); plt.legend()\n\n    plt.subplot(1,2,2)\n    plt.plot(h1.history['loss'], label='Phase1 Train')\n    plt.plot(h1.history['val_loss'], label='Phase1 Val')\n    plt.plot(x2, h2.history['loss'], label='Phase2 Train')\n    plt.plot(x2, h2.history['val_loss'], label='Phase2 Val')\n    plt.title('Loss'); plt.xlabel('Epoch'); plt.legend()\n    plt.tight_layout(); plt.show()\n\nplot_history(history1, history2)\ntest_loss, test_acc = model.evaluate(val_ds)\nprint(f\"\\nFinal Validation Accuracy: {test_acc}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-30T18:41:52.397536Z","iopub.execute_input":"2025-04-30T18:41:52.398213Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded to: /kaggle/input/adni-for-ad-progression\n\nCollecting image paths and labels…\n\nClass distribution:\n  CN: 4077 images\n  MCI: 4073 images\n  EMCI: 3958 images\n  LMCI: 4074 images\n  AD: 4075 images\n\nTraining:   16205 images\nValidation: 4052 images\nDownloading data from https://github.com/shkarupa-alex/tfswin/releases/download/3.0.0/swin_tiny_patch4_window7_224_22k.h5\n\u001b[1m177485300/177485300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n\nPhase 1: Training with Swin frozen\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746038567.316768      90 service.cc:148] XLA service 0x7d1af4025550 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746038567.317624      90 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1746038572.998186      90 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1746038608.896080      90 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 644ms/step - accuracy: 0.2039 - loss: 1.8865 - val_accuracy: 0.2011 - val_loss: 1.7813\nEpoch 2/10\n\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 357ms/step - accuracy: 0.2011 - loss: 1.8954 - val_accuracy: 0.2011 - val_loss: 1.7813\nEpoch 3/10\n\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 356ms/step - accuracy: 0.2055 - loss: 1.8879 - val_accuracy: 0.2011 - val_loss: 1.7813\nEpoch 4/10\n\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 356ms/step - accuracy: 0.2026 - loss: 1.8970 - val_accuracy: 0.2011 - val_loss: 1.7813\n\nPhase 2: Fine-tuning full Swin\nEpoch 1/10\n\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 477ms/step - accuracy: 0.2023 - loss: 1.8993","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}